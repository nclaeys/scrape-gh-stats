name: Scrape and Commit Data

on:
  workflow_dispatch:    # allow manual runs
  schedule:
    - cron: "0 8 * * 1" # every Monday at 08:00 UTC

jobs:
  scrape:
    runs-on: ubuntu-latest
    permissions:
      contents: read

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0   # needed so commits can be pushed back

      - name: Install GitHub CLI and jq
        run: sudo apt-get install -y gh jq

      - name: Authenticate using PAT
        run: echo "${{ secrets.PAT_TOKEN }}" | gh auth login --with-token

      - name: Run scrape script
        run: |
          bash ./scrape-data-gh.sh datamindedbe/demo-upcloud-data-platform
          bash ./scrape-data-gh.sh datamindedbe/eu-data-platform

      - name: Commit changes
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "niels.claeys@gmail.com"

          if [[ -n "$(git status --porcelain)" ]]; then
            git add -A
            git commit -m "chore: update scraped data"
            git push
          else
            echo "No changes to commit."
          fi